# -*- coding: utf-8 -*-
"""analyse data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QkdB-ChyzXVWAjwRU5Ip9L8c6P1UNqa1
"""

#Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#For ignoring warning
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
df=pd.read_csv('/content/drive/MyDrive/input/Analyse/student_exam_scores.csv')
df

df.shape

df.duplicated().sum()

#Removing Duplicates
df=df.drop_duplicates()

#Checking for null values
df.isnull().sum()

df.info()

df.describe()

from sklearn import preprocessing
le=preprocessing.LabelEncoder()
df['student_id']=le.fit_transform(df['student_id'])

#Let's check what's happened now
df

df.info()

#Let's check the distributaion of Target variable.
sns.histplot(x='exam_score', data=df, kde=True)
plt.title('Target Distribution');

df['exam_score'].value_counts()

# function for plotting
def plot(col, df=df):
    return df.groupby(col)['LUNG_CANCER'].value_counts(normalize=True).unstack().plot(kind='bar', figsize=(8,5))

# function for plotting
def plot(col, df=df):
    return df.groupby(col)['exam_score'].value_counts(normalize=True).unstack().plot(kind='bar', figsize=(8,5))

plot('student_id')

# Importing necessary libraries for plotting (if not already imported)
import seaborn as sns
import matplotlib.pyplot as plt

# Finding Correlation for the current dataframe, considering only numeric columns
cn = df.corr(numeric_only=True)

# Plotting the correlation heatmap
cmap = sns.diverging_palette(260, -10, s=50, l=75, n=6, as_cmap=True)
plt.subplots(figsize=(12, 10)) # Adjusted figsize for better readability
sns.heatmap(cn, cmap=cmap, annot=True, square=True, fmt=".2f") # fmt for better annotation display
plt.title('Correlation Matrix of Student Exam Scores Data')
plt.show()

#Finding Correlation
cn=df.corr(numeric_only=True)
cn

#Correlation
cmap=sns.diverging_palette(260,-10,s=50, l=75, n=6,
as_cmap=True)
plt.subplots(figsize=(18,18))
sns.heatmap(cn,cmap=cmap,annot=True, square=True)
plt.show()

kot = cn[cn>=.40]
plt.figure(figsize=(12,8))
sns.heatmap(kot, cmap="Blues")

df_new['student_id']=df_new['student_id']*df_new['exam_score']
df_new

# Create df_new as a copy of df to ensure it is defined
df_new = df.copy()

df_new

#Splitting independent and dependent variables
X = df_new.drop('exam_score', axis = 1)
y = df_new['exam_score']

len(X)

#Splitting data for training and testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.25, random_state=0)

#Fitting training data to the model
from sklearn.linear_model import LinearRegression # Changed from LogisticRegression to LinearRegression
lr_model=LinearRegression() # Changed from LogisticRegression() to LinearRegression()
lr_model.fit(X_train, y_train)

#Predicting result using testing data
y_lr_pred= lr_model.predict(X_test)
y_lr_pred

#Model accuracy using regression metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np # Import numpy

mae = mean_absolute_error(y_test, y_lr_pred)
mse = mean_squared_error(y_test, y_lr_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_lr_pred)

print(f'Mean Absolute Error (MAE): {mae:.2f}')
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
print(f'R-squared (R2): {r2:.2f}')

#Fitting training data to the model
from sklearn.tree import DecisionTreeRegressor
dt_model= DecisionTreeRegressor(criterion='squared_error', random_state=0)
dt_model.fit(X_train, y_train)

#Predicting result using testing data
y_dt_pred= dt_model.predict(X_test)
y_dt_pred

#Model accuracy
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
mae = mean_absolute_error(y_test, y_dt_pred)
mse = mean_squared_error(y_test, y_dt_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_dt_pred)
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

#Fitting K-NN Regressor to the training set
from sklearn.neighbors import KNeighborsRegressor # Changed to KNeighborsRegressor
knn_model= KNeighborsRegressor(n_neighbors=5) # No 'metric' or 'p' needed for KNeighborsRegressor by default with Euclidean distance
knn_model.fit(X_train, y_train)

# Predicting result using testing data
y_knn_pred = knn_model.predict(X_test)

# Model accuracy using regression metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_knn_pred)
mse = mean_squared_error(y_test, y_knn_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_knn_pred)

print(f'Mean Absolute Error (MAE): {mae:.2f}')
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
print(f'R-squared (R2): {r2:.2f}')

#Predicting result using testing data
y_knn_pred= knn_model.predict(X_test)
y_knn_pred